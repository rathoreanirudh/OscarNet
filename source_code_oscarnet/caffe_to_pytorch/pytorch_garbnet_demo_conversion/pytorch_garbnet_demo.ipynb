{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "import os\n",
    "from pylab import *\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 (96, 3, 11, 11)\n",
      "conv2 (256, 48, 5, 5)\n",
      "conv3 (384, 256, 3, 3)\n",
      "conv4 (384, 192, 3, 3)\n",
      "conv5 (256, 192, 3, 3)\n",
      "fc6_gb_conv (512, 256, 6, 6)\n",
      "fc7_gb_conv (256, 512, 1, 1)\n",
      "fc8_gb_conv (2, 256, 1, 1)\n",
      "\n",
      "\n",
      "data (1, 3, 227, 227)\n",
      "conv1 (1, 96, 55, 55)\n",
      "pool1 (1, 96, 27, 27)\n",
      "conv2 (1, 256, 27, 27)\n",
      "pool2 (1, 256, 13, 13)\n",
      "conv3 (1, 384, 13, 13)\n",
      "conv4 (1, 384, 13, 13)\n",
      "conv5 (1, 256, 13, 13)\n",
      "pool5 (1, 256, 6, 6)\n",
      "fc6_gb_conv (1, 512, 1, 1)\n",
      "fc7_gb_conv (1, 256, 1, 1)\n",
      "fc8_gb_conv (1, 2, 1, 1)\n",
      "prob (1, 2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "mean_filename='SpotGarbage_GarbNet/garbnet_mean.binaryproto'\n",
    "deploy_filename = 'SpotGarbage_GarbNet/deploy_garbnet.prototxt'\n",
    "caffemodel_file = 'SpotGarbage_GarbNet/garbnet_fcn.caffemodel'\n",
    "\n",
    "proto_data = open(mean_filename, \"rb\").read()\n",
    "a = caffe.io.caffe_pb2.BlobProto.FromString(proto_data)\n",
    "mean  = caffe.io.blobproto_to_array(a)[0]\n",
    "\n",
    "net = caffe.Net(deploy_filename,caffemodel_file,caffe.TEST)\n",
    "for item,blob in net.params.items():\n",
    "    print(item,blob[0].data.shape)\n",
    "    \n",
    "print('\\n')\n",
    "for item,blob in net.blobs.items():\n",
    "    print(item,blob.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KitModel(\n",
       "  (conv1): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "  (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), groups=2)\n",
       "  (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=2)\n",
       "  (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), groups=2)\n",
       "  (fc6_gb_conv): Conv2d(256, 512, kernel_size=(6, 6), stride=(2, 2))\n",
       "  (fc7_gb_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc8_gb_conv): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import imp\n",
    "MainModel = imp.load_source('MainModel', \"SpotGarbage_GarbNet/pytorch_garbnet.py\")\n",
    "\n",
    "the_model = torch.load(\"SpotGarbage_GarbNet/pytorch_garbnet.pth\")\n",
    "the_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    #1.resize for the network 277x277 (as garbdemo instructed)\n",
    "    #2.convert to tensor (gpu array)\n",
    "    #3.will normalize data between -1 and 1\n",
    "    transform = torchvision.transforms.Compose(\n",
    "                    [torchvision.transforms.Resize((277,277)),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                     torchvision.transforms.Normalize(\n",
    "                         (108.39786852, 115.89358715, 119.99467375), (1.0, 1.0, 1.0)\n",
    "                     )\n",
    "                    ])\n",
    "    \n",
    "    #to import images from the disk\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=path,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    #creates an iterable for the dataset to use in a forloop\n",
    "    dataset_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=2,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return dataset_loader\n",
    "\n",
    "'''\n",
    "image directory must be in the same location of this script\n",
    "images/garbage/img0.jpg,img1.jpg,img2.jpg\n",
    "images/notgarbage/img3.jpg,img4.jpg\n",
    "\n",
    "pytorch will label the classes based on the number of folders there are.\n",
    "garbage is folder 0 so class label is 0\n",
    "notgarbage is folder 1 so class label is 1\n",
    "\n",
    "only need the directory where all class folders are for argument\n",
    "'''\n",
    "dataset = load_dataset('images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      " 0.Garbage \n",
      " 1.Not Garbage\n",
      "\n",
      "Tensor (image to array) size:  torch.Size([1, 3, 277, 277]) Target label:  tensor([0])\n",
      "\n",
      "Tensor (image to array) size:  torch.Size([1, 3, 277, 277]) Target label:  tensor([0])\n",
      "\n",
      "Tensor (image to array) size:  torch.Size([1, 3, 277, 277]) Target label:  tensor([0])\n",
      "\n",
      "Tensor (image to array) size:  torch.Size([1, 3, 277, 277]) Target label:  tensor([1])\n",
      "\n",
      "Tensor (image to array) size:  torch.Size([1, 3, 277, 277]) Target label:  tensor([1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Labels: \\n 0.Garbage \\n 1.Not Garbage\\n')\n",
    "for data, target in dataset:\n",
    "    #prints [batchsize, channel size (RGB->3), height, width], label \n",
    "    print('Tensor (image to array) size: ', data.size(), 'Target label: ', target,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[2.0495e-08, 1.3384e-08],\n",
      "          [9.6618e-09, 7.9385e-09]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(3) tensor([0])\n",
      "tensor([[[[1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[2.1155e-08, 1.5461e-08],\n",
      "          [1.0189e-08, 8.2807e-09]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(3) tensor([0])\n",
      "tensor([[[[1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[2.3340e-08, 1.6308e-08],\n",
      "          [1.1409e-08, 9.6301e-09]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(3) tensor([0])\n",
      "tensor([[[[1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[2.8744e-08, 1.9176e-08],\n",
      "          [1.4363e-08, 1.1266e-08]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(3) tensor([1])\n",
      "tensor([[[[1.0000e+00, 1.0000e+00],\n",
      "          [1.0000e+00, 1.0000e+00]],\n",
      "\n",
      "         [[2.8395e-08, 1.9204e-08],\n",
      "          [1.5104e-08, 1.1878e-08]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(3) tensor([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SpotGarbage_GarbNet/pytorch_garbnet.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob            = F.softmax(fc8_gb_conv)\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "for data, target in dataset:\n",
    "    pred = the_model(data)\n",
    "    print(pred)\n",
    "    print(pred.argmax(), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherImages(folder,imageNames=None):\n",
    "    images = []\n",
    "    names = []\n",
    "    files = os.listdir(folder)\n",
    "    total = len(files)\n",
    "    #print('Total %d images in folder %s' % (total,folder))\n",
    "    for i in os.listdir(folder):\n",
    "        try:\n",
    "            if imageNames is None or i in imageNames:\n",
    "                example_image = folder+'/'+i\n",
    "                input_image = Image.open(example_image)\n",
    "                images.append(input_image)\n",
    "                names.append(i)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return images,names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resizeForFCN(image,size):\n",
    "    w,h = image.size\n",
    "    if w<h:\n",
    "        return image.resize((int(227*size),int((227*h*size)/w))) #227x227 is input for regular CNN\n",
    "    else:\n",
    "        return image.resize((int((227*w*size)/h),int(227*size)))\n",
    "    \n",
    "def getSegmentedImage(test_image, probMap,thresh):\n",
    "    kernel = np.ones((6,6),np.uint8)\n",
    "    wt,ht = test_image.size\n",
    "    out_bn = np.zeros((ht,wt),dtype=uint8)\n",
    "    res = 0\n",
    "    for h in range(probMap.shape[0]):\n",
    "        \n",
    "        for k in range(probMap.shape[1]):\n",
    "            if probMap[h,k] > thresh:\n",
    "                res += 1\n",
    "                x1 = h*62 #stride 2 at fc6_gb_conv equivalent to 62 pixels stride in input\n",
    "                y1 = k*62\n",
    "                for hoff in range(x1,227+x1):\n",
    "                    if hoff < out_bn.shape[0]:\n",
    "                        for koff in range(y1,227+y1):\n",
    "                            if koff < out_bn.shape[1]:\n",
    "                                out_bn[hoff,koff] = 255\n",
    "    print(\"count \",res)\n",
    "    edge = cv2.Canny(out_bn,200,250)\n",
    "    box = cv2.dilate(edge,kernel,iterations = 3)\n",
    "    \n",
    "    or_im_ar = np.array(test_image)\n",
    "    or_im_ar[:,:,1] = (or_im_ar[:,:,1] | box)\n",
    "    or_im_ar[:,:,2] = or_im_ar[:,:,2] * box + or_im_ar[:,:,2]\n",
    "    or_im_ar[:,:,0] = or_im_ar[:,:,0] * box + or_im_ar[:,:,0]\n",
    "    \n",
    "    return Image.fromarray(or_im_ar)\n",
    "    \n",
    "    \n",
    "def getPredictionsFor(images,names,size,thresh,output_folder):\n",
    "    for i in range(len(images)):\n",
    "#         try:\n",
    "            test_image = resizeForFCN(images[i],size)\n",
    "            \n",
    "            in_ = np.array(test_image,dtype = np.float32)\n",
    "            in_ = in_[:,:,::-1]\n",
    "            print('min before scaling',np.amin(in_))\n",
    "            print('max before scaling',np.amax(in_))\n",
    "            in_ -= np.array(mean.mean(1).mean(1))\n",
    "            print('mean scaling', np.array(mean.mean(1).mean(1)))\n",
    "            print('min after scaling',np.amin(in_))\n",
    "            print('max after scaling',np.amax(in_))\n",
    "            in_ = in_.transpose((2,0,1))\n",
    "            if i ==0:\n",
    "                print(in_)\n",
    "            #print('in_',in_)\n",
    "            net.blobs['data'].reshape(1,*in_.shape)\n",
    "            net.blobs['data'].data[...] = in_\n",
    "            net.forward()\n",
    "            #print(net.blobs['prob'].data[0,1])\n",
    "            probMap =net.blobs['prob'].data[0,1]\n",
    "            print('probmap: ',probMap.shape)\n",
    "            print( names[i]+'...',)\n",
    "            if len(np.where(probMap>thresh)[0]) > 0:\n",
    "                print( 'Garbage!')\n",
    "            else:\n",
    "                print('Not Garbage!')\n",
    "            \n",
    "            out_ = getSegmentedImage(test_image, probMap,thresh)\n",
    "            out_.save(output_folder + '/output_' + names[i])\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=550x367 at 0x7F7957A14510>\n",
      "min before scaling 0.0\n",
      "max before scaling 255.0\n",
      "mean scaling [108.39786852 115.89358715 119.9946737 ]\n",
      "min after scaling -119.994675\n",
      "max after scaling 146.60213\n",
      "[[[-31.39787  -31.39787  -67.397865 ... -38.39787  -37.39787  -37.39787 ]\n",
      "  [-31.39787  -31.39787  -67.397865 ... -38.39787  -37.39787  -37.39787 ]\n",
      "  [-28.39787  -28.39787  -72.397865 ... -38.39787  -36.39787  -36.39787 ]\n",
      "  ...\n",
      "  [ 40.60213   40.60213   28.60213  ...  61.60213   44.60213   44.60213 ]\n",
      "  [ 66.602135  66.602135  44.60213  ...  57.60213   47.60213   47.60213 ]\n",
      "  [ 66.602135  66.602135  44.60213  ...  57.60213   47.60213   47.60213 ]]\n",
      "\n",
      " [[-33.89359  -33.89359  -69.893585 ... -33.89359  -32.89359  -32.89359 ]\n",
      "  [-33.89359  -33.89359  -69.893585 ... -33.89359  -32.89359  -32.89359 ]\n",
      "  [-30.893587 -30.893587 -75.893585 ... -33.89359  -31.893587 -31.893587]\n",
      "  ...\n",
      "  [ 43.10641   43.10641   31.106413 ...  55.10641   38.10641   38.10641 ]\n",
      "  [ 69.106415  69.106415  47.10641  ...  51.10641   41.10641   41.10641 ]\n",
      "  [ 69.106415  69.106415  47.10641  ...  51.10641   41.10641   41.10641 ]]\n",
      "\n",
      " [[-39.994675 -39.994675 -70.994675 ...  50.005325  51.005325  51.005325]\n",
      "  [-39.994675 -39.994675 -70.994675 ...  50.005325  51.005325  51.005325]\n",
      "  [-35.994675 -35.994675 -74.994675 ...  50.005325  52.005325  52.005325]\n",
      "  ...\n",
      "  [ 69.005325  69.005325  57.005325 ...  85.005325  68.005325  68.005325]\n",
      "  [ 95.005325  95.005325  73.005325 ...  81.005325  71.005325  71.005325]\n",
      "  [ 95.005325  95.005325  73.005325 ...  81.005325  71.005325  71.005325]]]\n",
      "probmap:  (12, 19)\n",
      "sample_2_garbage.jpg...\n",
      "Garbage!\n",
      "count  26\n",
      "min before scaling 0.0\n",
      "max before scaling 255.0\n",
      "mean scaling [108.39786852 115.89358715 119.9946737 ]\n",
      "min after scaling -119.994675\n",
      "max after scaling 146.60213\n",
      "probmap:  (12, 16)\n",
      "sample_1_garbage.jpg...\n",
      "Garbage!\n",
      "count  52\n",
      "min before scaling 0.0\n",
      "max before scaling 255.0\n",
      "mean scaling [108.39786852 115.89358715 119.9946737 ]\n",
      "min after scaling -119.994675\n",
      "max after scaling 146.60213\n",
      "probmap:  (12, 21)\n",
      "sample_3_garbage.jpg...\n",
      "Garbage!\n",
      "count  70\n"
     ]
    }
   ],
   "source": [
    "#specify 'input' folder containing images for prediction\n",
    "images,names = gatherImages('images/garbage')\n",
    "print(images[0])\n",
    "#images[0].show()\n",
    "#specify 'output' folder to store segmented predictions\n",
    "getPredictionsFor(images,names,4,0.999,'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input/sample_2_garbage.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f67fdcfd0252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input/sample_2_garbage.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/output_sample_2_garbage.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input/sample_2_garbage.jpg'"
     ]
    }
   ],
   "source": [
    "imshow(Image.open('input/sample_2_garbage.jpg'))\n",
    "figure()\n",
    "imshow(Image.open('output/output_sample_2_garbage.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
